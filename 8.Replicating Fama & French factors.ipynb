{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating Fama & French factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fama and French three-factor model [Fama1993](https://doi.org/2329112) is a cornerstone of asset pricing. On top of the market factor represented by the traditional CAPM beta, the model includes the size and value factors. We introduce both factors in the previous chapter, and their definition remains the same. Size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short the low book-to-market counterparts. In this chapter, we also want to show the main idea of how to replicate these significant factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CRSP and Compustat as data sources, as we need exactly the same variables to compute the size and value factors in the way Fama and French do it. Hence, there is nothing new below and we only load data from our `SQLite`-database introduced in our chapter on *\"Accessing & managing financial data\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import timedelta\n",
    "# Read sqlite query results into a pandas DataFrame\n",
    "tidy_finance = sqlite3.connect(\"D:/Tidy/tidyfinance.sqlite\")\n",
    "crsp_monthly = pd.read_sql_query(\"SELECT * from crsp_monthly\", tidy_finance)\n",
    "factors_ff_monthly = pd.read_sql_query(\"SELECT * from factors_ff_monthly\", tidy_finance)\n",
    "compustat = pd.read_sql_query(\"SELECT * from compustat\", tidy_finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ff=pd.merge(crsp_monthly,factors_ff_monthly,left_on='month',right_on='month',how='left')[['permno', 'gvkey', 'month', 'ret_excess', 'mkt_excess','mktcap', 'mktcap_lag', 'exchange']]\n",
    "data_ff=data_ff.assign(month=pd.to_datetime(data_ff['month']),permno = data_ff['permno'].astype(int))\n",
    "data_ff=data_ff.dropna()\n",
    "be = compustat[[ 'gvkey', 'datadate', 'be']].dropna()\n",
    "be['month']=pd.to_datetime(be['datadate']) + timedelta(days=1) - pd.DateOffset(months=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet when we start merging our data set for computing the premiums, there are a few differences to the previous chapter. First, Fama and French form their portfolios in June of year $t$, whereby the returns of July are the first monthly return for the respective portfolio. For firm size, they consequently use the market capitalization recorded for June. It is then held constant until June of year $t+1$.\n",
    "\n",
    "Second, Fama and French also have a different protocol for computing the book-to-market ratio. They use market equity as of the end of year $t - 1$ and the book equity reported in year $t-1$, i.e., the `datadate` is within the last year. Hence, the book-to-market ratio can be based on accounting information that is up to 18 months old. Market equity also does not necessarily reflect the same time point as book equity.\n",
    "\n",
    "To implement all these time lags, we again employ the temporary `sorting_date`-column. Notice that when we combine the information, we want to have a single observation per year and stock since we are only interested in computing the breakpoints held constant for the entire year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_ff=data_ff.loc[data_ff.month.dt.month == 6].assign(sorting_date = data_ff.month +pd.DateOffset(months=1),me_ff = data_ff.mktcap)[['permno', 'sorting_date', 'me_ff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_ff_dec=data_ff.loc[data_ff.month.dt.month == 12].assign(sorting_date =  data_ff.loc[data_ff.month.dt.month == 12].month.dt.year.apply(lambda x: pd.to_datetime(str(x+1)+'0701')),bm_me = data_ff.mktcap) [['permno', 'gvkey', 'sorting_date', 'bm_me']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_ff=be.assign(sorting_date = be.month.dt.year.apply(lambda x: pd.to_datetime(str(x+1)+'0701')), bm_be = be.be)[['gvkey', 'sorting_date', 'bm_be']].dropna().merge(me_ff_dec, left_on = [\"gvkey\", \"sorting_date\"],right_on = [\"gvkey\", \"sorting_date\"],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_ff=bm_ff.assign(bm_ff = bm_ff.bm_be / bm_ff.bm_me)[['permno', 'sorting_date', 'bm_ff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_ff = me_ff.merge(bm_ff, left_on=[\"permno\", \"sorting_date\"],right_on = [\"permno\", \"sorting_date\"],how='inner').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio sorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct our portfolios with an adjusted `assign_portfolio()` function. Fama and French rely on NYSE-specific breakpoints, they form two portfolios in the size dimension at the median and three portfolios in the dimension of book-to-market at the 30%- and 70%-percentiles, and they use independent sorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios_ff=variables_ff.merge(data_ff, left_on = [\"permno\" , \"sorting_date\"],right_on = [\"permno\" , \"month\"],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_sz=portfolios_ff.loc[portfolios_ff['exchange'].isin([\"NYSE\"])].groupby(['month'])['me_ff'].median().to_frame().reset_index().rename(columns={'me_ff':'sizemedn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_bm=portfolios_ff.loc[portfolios_ff['exchange'].isin([\"NYSE\"])].groupby([\"month\"])['bm_ff'].describe(percentiles=[0.3, 0.7]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_bm=nyse_bm[['month','30%','70%']].rename(columns={'30%':'bm30', '70%':'bm70'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_breaks = pd.merge(nyse_sz, nyse_bm, how='inner', on=['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccm1_jun = pd.merge(portfolios_ff, nyse_breaks, how='left', on=['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sz_bucket(row):\n",
    "    if row['me_ff'] == np.nan:\n",
    "        value=''\n",
    "    elif row['me_ff']<=row['sizemedn']:\n",
    "        value=1\n",
    "    else:\n",
    "        value=2\n",
    "    return value\n",
    "\n",
    "def bm_bucket(row):\n",
    "    if 0<=row['bm_ff']<=row['bm30']:\n",
    "        value = 1\n",
    "    elif row['bm_ff']<=row['bm70']:\n",
    "        value=2\n",
    "    elif row['bm_ff']>row['bm70']:\n",
    "        value=3\n",
    "    else:\n",
    "        value=''\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccm1_jun['portfolio_me']=np.where((ccm1_jun['bm_ff']>0)&(ccm1_jun['me_ff']>0), ccm1_jun.apply(sz_bucket, axis=1), '')\n",
    "ccm1_jun['portfolio_bm']=np.where((ccm1_jun['bm_ff']>0)&(ccm1_jun['me_ff']>0), ccm1_jun.apply(bm_bucket, axis=1), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios_ff=ccm1_jun[['permno', 'sorting_date', 'portfolio_me', 'portfolio_bm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we merge the portfolios to the return data for the rest of the year. To implement this step, we create a new column `sorting_date` in our return data by setting the date to sort on to July of $t-1$ if the month is June (of year $t$) or earlier or to July of year $t$ if the month is July or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ff.loc[data_ff.month.dt.month <= 6,'sorting_date'] = data_ff.month.dt.year.apply(lambda x: pd.to_datetime(str(x-1)+'0701'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ff.loc[data_ff.month.dt.month >= 7,'sorting_date'] = data_ff.month.dt.year.apply(lambda x: pd.to_datetime(str(x)+'0701'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios_ff=data_ff.merge(portfolios_ff, left_on=[\"permno\",'sorting_date'],right_on=[\"permno\",'sorting_date'],how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fama and French factor returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama and French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios_ff['portfolio']=portfolios_ff.apply(lambda x:str(int(x['portfolio_me'])) + '-' + str(int(x['portfolio_bm'])),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios_ff=portfolios_ff.groupby(['month','portfolio']).apply(lambda x: pd.Series([np.average(x['ret_excess'], weights=x['mktcap_lag'])], \n",
    "                                                                index=['ret'])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios_ff['portfolio_me']=portfolios_ff.portfolio.apply(lambda x:x[0])\n",
    "portfolios_ff['portfolio_bm']=portfolios_ff.portfolio.apply(lambda x:x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "smb_replicated=portfolios_ff.groupby(['month','portfolio_me']).ret.mean().unstack()\n",
    "smb_replicated=(smb_replicated['1']-smb_replicated['2']).rename('smb_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hml_replicated=portfolios_ff.groupby(['month','portfolio_bm']).ret.mean().unstack()\n",
    "hml_replicated=(hml_replicated['3']-hml_replicated['1']).rename('hml_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we replicated the size and value premiums following the procedure outlined by Fama and French. However, we did not follow their procedure strictly. The final question is then: how close did we get? We answer this question by looking at the two time-series estimates in a regression analysis using `statsmodels.api`. If we did a good job, then we should see a non-significant intercept (rejecting the notion of systematic error), a coefficient close to 1 (indicating a high correlation), and an adjusted R-squared close to 1 (indicating a high proportion of explained variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_ff_monthly['month']=pd.to_datetime(factors_ff_monthly['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = factors_ff_monthly.merge( hml_replicated, left_on = [\"month\"],right_on=[\"month\"]).merge( smb_replicated, left_on = [\"month\"],right_on=[\"month\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['hml_r']=test['hml_r'].round(decimals = 4)\n",
    "test['smb_r']=test['smb_r'].round(decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.984\n",
      "Model:                            OLS   Adj. R-squared:                  0.984\n",
      "Method:                 Least Squares   F-statistic:                 4.498e+04\n",
      "Date:                Tue, 23 Aug 2022   Prob (F-statistic):               0.00\n",
      "Time:                        16:16:09   Log-Likelihood:                 2971.4\n",
      "No. Observations:                 714   AIC:                            -5939.\n",
      "Df Residuals:                     712   BIC:                            -5930.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0002      0.000     -1.250      0.212      -0.000       0.000\n",
      "x1             0.9949      0.005    212.076      0.000       0.986       1.004\n",
      "==============================================================================\n",
      "Omnibus:                      167.653   Durbin-Watson:                   2.212\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1927.342\n",
      "Skew:                          -0.697   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.927   Cond. No.                         33.2\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "y=test['smb'].values\n",
    "X=test['smb_r'].values\n",
    "X=sm.add_constant(X)\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.950\n",
      "Model:                            OLS   Adj. R-squared:                  0.950\n",
      "Method:                 Least Squares   F-statistic:                 1.359e+04\n",
      "Date:                Tue, 23 Aug 2022   Prob (F-statistic):               0.00\n",
      "Time:                        16:16:13   Log-Likelihood:                 2603.1\n",
      "No. Observations:                 714   AIC:                            -5202.\n",
      "Df Residuals:                     712   BIC:                            -5193.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0004      0.000      1.739      0.083   -5.34e-05       0.001\n",
      "x1             0.9608      0.008    116.590      0.000       0.945       0.977\n",
      "==============================================================================\n",
      "Omnibus:                      144.723   Durbin-Watson:                   2.137\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              873.902\n",
      "Skew:                           0.757   Prob(JB):                    1.72e-190\n",
      "Kurtosis:                       8.204   Cond. No.                         34.8\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "y=test['hml'].values\n",
    "X=test['hml_r'].values\n",
    "X=sm.add_constant(X)\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evidence hence allows us to conclude that we did a relatively good job in replicating the original Fama-French premiums, although we cannot see their underlying code. \n",
    "From our perspective, a perfect match is only possible with additional information from the maintainers of the original data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "670636528462d7cb6f82d90df507c1e6f4b23efee7fba44815f39cb55d1b33a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
